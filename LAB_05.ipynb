{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9SqUSTGCDw86zSGjM9iFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51395/Reinforcement-learning/blob/main/LAB_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import argparse  # for command-line arguments\n",
        "\n",
        "# Try Gymnasium first; fallback to Gym\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYMN = \"gymnasium\"\n",
        "except Exception:\n",
        "    import gym\n",
        "    GYMN = \"gym\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# ---------------------------\n",
        "# Q-Network\n",
        "# ---------------------------\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Replay Buffer\n",
        "# ---------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Loop\n",
        "# ---------------------------\n",
        "def train(env_id, total_steps, start_learning, buffer_size, batch_size,\n",
        "          gamma, lr, target_update, eps_start, eps_end, eps_decay_steps):\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    q_net = QNetwork(state_dim, action_dim).to(device)\n",
        "    target_net = QNetwork(state_dim, action_dim).to(device)\n",
        "    target_net.load_state_dict(q_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
        "    replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "    epsilon = eps_start\n",
        "    epsilon_decay = (eps_start - eps_end) / eps_decay_steps\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(1, total_steps + 1):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                q_values = q_net(state_tensor)\n",
        "                action = q_values.argmax().item()\n",
        "\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        replay_buffer.push(state, action, reward, next_state, done or truncated)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # If episode ends\n",
        "        if done or truncated:\n",
        "            print(f\"Step {step}, Episode Reward: {total_reward}\")\n",
        "            state, _ = env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "        # Training step\n",
        "        if step > start_learning and len(replay_buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "            states = torch.FloatTensor(states).to(device)\n",
        "            actions = torch.LongTensor(actions).to(device)\n",
        "            rewards = torch.FloatTensor(rewards).to(device)\n",
        "            next_states = torch.FloatTensor(next_states).to(device)\n",
        "            dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "            q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            with torch.no_grad():\n",
        "                max_next_q = target_net(next_states).max(1)[0]\n",
        "                target = rewards + gamma * max_next_q * (1 - dones)\n",
        "\n",
        "            loss = F.mse_loss(q_values, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if step % target_update == 0:\n",
        "            target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        # Decay epsilon\n",
        "        if epsilon > eps_end:\n",
        "            epsilon -= epsilon_decay\n",
        "            epsilon = max(eps_end, epsilon)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# ---------------------------\n",
        "# Main with argparse\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--env\", type=str, default=\"CartPole-v1\")\n",
        "    parser.add_argument(\"--total-steps\", type=int, default=10000)\n",
        "    parser.add_argument(\"--start-learning\", type=int, default=1000)\n",
        "    parser.add_argument(\"--buffer-size\", type=int, default=10000)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--target-update\", type=int, default=1000)\n",
        "    parser.add_argument(\"--eps-start\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--eps-end\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--eps-decay-steps\", type=int, default=10000)\n",
        "\n",
        "    # âœ… Fix for Jupyter/Colab\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    train(env_id=args.env,\n",
        "          total_steps=args.total_steps,\n",
        "          start_learning=args.start_learning,\n",
        "          buffer_size=args.buffer_size,\n",
        "          batch_size=args.batch_size,\n",
        "          gamma=args.gamma,\n",
        "          lr=args.lr,\n",
        "          target_update=args.target_update,\n",
        "          eps_start=args.eps_start,\n",
        "          eps_end=args.eps_end,\n",
        "          eps_decay_steps=args.eps_decay_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q1owSFH4rB7",
        "outputId": "b0e40570-0c13-4c8a-9248-fbf6ed7a47c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 15, Episode Reward: 15.0\n",
            "Step 30, Episode Reward: 15.0\n",
            "Step 46, Episode Reward: 16.0\n",
            "Step 76, Episode Reward: 30.0\n",
            "Step 90, Episode Reward: 14.0\n",
            "Step 110, Episode Reward: 20.0\n",
            "Step 126, Episode Reward: 16.0\n",
            "Step 143, Episode Reward: 17.0\n",
            "Step 158, Episode Reward: 15.0\n",
            "Step 173, Episode Reward: 15.0\n",
            "Step 192, Episode Reward: 19.0\n",
            "Step 203, Episode Reward: 11.0\n",
            "Step 231, Episode Reward: 28.0\n",
            "Step 263, Episode Reward: 32.0\n",
            "Step 276, Episode Reward: 13.0\n",
            "Step 290, Episode Reward: 14.0\n",
            "Step 313, Episode Reward: 23.0\n",
            "Step 347, Episode Reward: 34.0\n",
            "Step 356, Episode Reward: 9.0\n",
            "Step 368, Episode Reward: 12.0\n",
            "Step 388, Episode Reward: 20.0\n",
            "Step 419, Episode Reward: 31.0\n",
            "Step 439, Episode Reward: 20.0\n",
            "Step 468, Episode Reward: 29.0\n",
            "Step 493, Episode Reward: 25.0\n",
            "Step 543, Episode Reward: 50.0\n",
            "Step 570, Episode Reward: 27.0\n",
            "Step 596, Episode Reward: 26.0\n",
            "Step 622, Episode Reward: 26.0\n",
            "Step 635, Episode Reward: 13.0\n",
            "Step 653, Episode Reward: 18.0\n",
            "Step 664, Episode Reward: 11.0\n",
            "Step 736, Episode Reward: 72.0\n",
            "Step 751, Episode Reward: 15.0\n",
            "Step 769, Episode Reward: 18.0\n",
            "Step 790, Episode Reward: 21.0\n",
            "Step 810, Episode Reward: 20.0\n",
            "Step 826, Episode Reward: 16.0\n",
            "Step 842, Episode Reward: 16.0\n",
            "Step 863, Episode Reward: 21.0\n",
            "Step 888, Episode Reward: 25.0\n",
            "Step 905, Episode Reward: 17.0\n",
            "Step 916, Episode Reward: 11.0\n",
            "Step 927, Episode Reward: 11.0\n",
            "Step 941, Episode Reward: 14.0\n",
            "Step 958, Episode Reward: 17.0\n",
            "Step 972, Episode Reward: 14.0\n",
            "Step 1000, Episode Reward: 28.0\n",
            "Step 1020, Episode Reward: 20.0\n",
            "Step 1033, Episode Reward: 13.0\n",
            "Step 1072, Episode Reward: 39.0\n",
            "Step 1085, Episode Reward: 13.0\n",
            "Step 1104, Episode Reward: 19.0\n",
            "Step 1157, Episode Reward: 53.0\n",
            "Step 1176, Episode Reward: 19.0\n",
            "Step 1200, Episode Reward: 24.0\n",
            "Step 1225, Episode Reward: 25.0\n",
            "Step 1257, Episode Reward: 32.0\n",
            "Step 1276, Episode Reward: 19.0\n",
            "Step 1294, Episode Reward: 18.0\n",
            "Step 1306, Episode Reward: 12.0\n",
            "Step 1314, Episode Reward: 8.0\n",
            "Step 1328, Episode Reward: 14.0\n",
            "Step 1338, Episode Reward: 10.0\n",
            "Step 1349, Episode Reward: 11.0\n",
            "Step 1360, Episode Reward: 11.0\n",
            "Step 1379, Episode Reward: 19.0\n",
            "Step 1396, Episode Reward: 17.0\n",
            "Step 1447, Episode Reward: 51.0\n",
            "Step 1465, Episode Reward: 18.0\n",
            "Step 1501, Episode Reward: 36.0\n",
            "Step 1520, Episode Reward: 19.0\n",
            "Step 1568, Episode Reward: 48.0\n",
            "Step 1580, Episode Reward: 12.0\n",
            "Step 1593, Episode Reward: 13.0\n",
            "Step 1606, Episode Reward: 13.0\n",
            "Step 1619, Episode Reward: 13.0\n",
            "Step 1631, Episode Reward: 12.0\n",
            "Step 1642, Episode Reward: 11.0\n",
            "Step 1655, Episode Reward: 13.0\n",
            "Step 1673, Episode Reward: 18.0\n",
            "Step 1705, Episode Reward: 32.0\n",
            "Step 1716, Episode Reward: 11.0\n",
            "Step 1740, Episode Reward: 24.0\n",
            "Step 1758, Episode Reward: 18.0\n",
            "Step 1782, Episode Reward: 24.0\n",
            "Step 1795, Episode Reward: 13.0\n",
            "Step 1808, Episode Reward: 13.0\n",
            "Step 1829, Episode Reward: 21.0\n",
            "Step 1848, Episode Reward: 19.0\n",
            "Step 1866, Episode Reward: 18.0\n",
            "Step 1882, Episode Reward: 16.0\n",
            "Step 1903, Episode Reward: 21.0\n",
            "Step 1933, Episode Reward: 30.0\n",
            "Step 1947, Episode Reward: 14.0\n",
            "Step 1969, Episode Reward: 22.0\n",
            "Step 1987, Episode Reward: 18.0\n",
            "Step 2012, Episode Reward: 25.0\n",
            "Step 2059, Episode Reward: 47.0\n",
            "Step 2073, Episode Reward: 14.0\n",
            "Step 2096, Episode Reward: 23.0\n",
            "Step 2111, Episode Reward: 15.0\n",
            "Step 2132, Episode Reward: 21.0\n",
            "Step 2144, Episode Reward: 12.0\n",
            "Step 2171, Episode Reward: 27.0\n",
            "Step 2184, Episode Reward: 13.0\n",
            "Step 2197, Episode Reward: 13.0\n",
            "Step 2208, Episode Reward: 11.0\n",
            "Step 2220, Episode Reward: 12.0\n",
            "Step 2235, Episode Reward: 15.0\n",
            "Step 2258, Episode Reward: 23.0\n",
            "Step 2276, Episode Reward: 18.0\n",
            "Step 2316, Episode Reward: 40.0\n",
            "Step 2337, Episode Reward: 21.0\n",
            "Step 2355, Episode Reward: 18.0\n",
            "Step 2377, Episode Reward: 22.0\n",
            "Step 2406, Episode Reward: 29.0\n",
            "Step 2431, Episode Reward: 25.0\n",
            "Step 2444, Episode Reward: 13.0\n",
            "Step 2477, Episode Reward: 33.0\n",
            "Step 2556, Episode Reward: 79.0\n",
            "Step 2567, Episode Reward: 11.0\n",
            "Step 2581, Episode Reward: 14.0\n",
            "Step 2598, Episode Reward: 17.0\n",
            "Step 2619, Episode Reward: 21.0\n",
            "Step 2654, Episode Reward: 35.0\n",
            "Step 2673, Episode Reward: 19.0\n",
            "Step 2690, Episode Reward: 17.0\n",
            "Step 2708, Episode Reward: 18.0\n",
            "Step 2730, Episode Reward: 22.0\n",
            "Step 2740, Episode Reward: 10.0\n",
            "Step 2764, Episode Reward: 24.0\n",
            "Step 2789, Episode Reward: 25.0\n",
            "Step 2798, Episode Reward: 9.0\n",
            "Step 2809, Episode Reward: 11.0\n",
            "Step 2827, Episode Reward: 18.0\n",
            "Step 2842, Episode Reward: 15.0\n",
            "Step 2878, Episode Reward: 36.0\n",
            "Step 2940, Episode Reward: 62.0\n",
            "Step 2956, Episode Reward: 16.0\n",
            "Step 2983, Episode Reward: 27.0\n",
            "Step 2996, Episode Reward: 13.0\n",
            "Step 3005, Episode Reward: 9.0\n",
            "Step 3019, Episode Reward: 14.0\n",
            "Step 3054, Episode Reward: 35.0\n",
            "Step 3067, Episode Reward: 13.0\n",
            "Step 3092, Episode Reward: 25.0\n",
            "Step 3132, Episode Reward: 40.0\n",
            "Step 3172, Episode Reward: 40.0\n",
            "Step 3208, Episode Reward: 36.0\n",
            "Step 3218, Episode Reward: 10.0\n",
            "Step 3241, Episode Reward: 23.0\n",
            "Step 3258, Episode Reward: 17.0\n",
            "Step 3275, Episode Reward: 17.0\n",
            "Step 3293, Episode Reward: 18.0\n",
            "Step 3308, Episode Reward: 15.0\n",
            "Step 3344, Episode Reward: 36.0\n",
            "Step 3361, Episode Reward: 17.0\n",
            "Step 3393, Episode Reward: 32.0\n",
            "Step 3414, Episode Reward: 21.0\n",
            "Step 3470, Episode Reward: 56.0\n",
            "Step 3496, Episode Reward: 26.0\n",
            "Step 3509, Episode Reward: 13.0\n",
            "Step 3523, Episode Reward: 14.0\n",
            "Step 3539, Episode Reward: 16.0\n",
            "Step 3571, Episode Reward: 32.0\n",
            "Step 3597, Episode Reward: 26.0\n",
            "Step 3678, Episode Reward: 81.0\n",
            "Step 3754, Episode Reward: 76.0\n",
            "Step 3785, Episode Reward: 31.0\n",
            "Step 3826, Episode Reward: 41.0\n",
            "Step 3850, Episode Reward: 24.0\n",
            "Step 3864, Episode Reward: 14.0\n",
            "Step 3875, Episode Reward: 11.0\n",
            "Step 3890, Episode Reward: 15.0\n",
            "Step 3900, Episode Reward: 10.0\n",
            "Step 3911, Episode Reward: 11.0\n",
            "Step 3924, Episode Reward: 13.0\n",
            "Step 3934, Episode Reward: 10.0\n",
            "Step 3947, Episode Reward: 13.0\n",
            "Step 3987, Episode Reward: 40.0\n",
            "Step 4018, Episode Reward: 31.0\n",
            "Step 4042, Episode Reward: 24.0\n",
            "Step 4056, Episode Reward: 14.0\n",
            "Step 4088, Episode Reward: 32.0\n",
            "Step 4120, Episode Reward: 32.0\n",
            "Step 4131, Episode Reward: 11.0\n",
            "Step 4189, Episode Reward: 58.0\n",
            "Step 4258, Episode Reward: 69.0\n",
            "Step 4292, Episode Reward: 34.0\n",
            "Step 4313, Episode Reward: 21.0\n",
            "Step 4324, Episode Reward: 11.0\n",
            "Step 4349, Episode Reward: 25.0\n",
            "Step 4373, Episode Reward: 24.0\n",
            "Step 4402, Episode Reward: 29.0\n",
            "Step 4418, Episode Reward: 16.0\n",
            "Step 4476, Episode Reward: 58.0\n",
            "Step 4505, Episode Reward: 29.0\n",
            "Step 4524, Episode Reward: 19.0\n",
            "Step 4561, Episode Reward: 37.0\n",
            "Step 4608, Episode Reward: 47.0\n",
            "Step 4627, Episode Reward: 19.0\n",
            "Step 4637, Episode Reward: 10.0\n",
            "Step 4651, Episode Reward: 14.0\n",
            "Step 4677, Episode Reward: 26.0\n",
            "Step 4702, Episode Reward: 25.0\n",
            "Step 4712, Episode Reward: 10.0\n",
            "Step 4721, Episode Reward: 9.0\n",
            "Step 4782, Episode Reward: 61.0\n",
            "Step 4863, Episode Reward: 81.0\n",
            "Step 4875, Episode Reward: 12.0\n",
            "Step 4888, Episode Reward: 13.0\n",
            "Step 4911, Episode Reward: 23.0\n",
            "Step 4935, Episode Reward: 24.0\n",
            "Step 4970, Episode Reward: 35.0\n",
            "Step 5033, Episode Reward: 63.0\n",
            "Step 5088, Episode Reward: 55.0\n",
            "Step 5126, Episode Reward: 38.0\n",
            "Step 5165, Episode Reward: 39.0\n",
            "Step 5198, Episode Reward: 33.0\n",
            "Step 5239, Episode Reward: 41.0\n",
            "Step 5259, Episode Reward: 20.0\n",
            "Step 5294, Episode Reward: 35.0\n",
            "Step 5316, Episode Reward: 22.0\n",
            "Step 5510, Episode Reward: 194.0\n",
            "Step 5574, Episode Reward: 64.0\n",
            "Step 5587, Episode Reward: 13.0\n",
            "Step 5625, Episode Reward: 38.0\n",
            "Step 5665, Episode Reward: 40.0\n",
            "Step 5764, Episode Reward: 99.0\n",
            "Step 5930, Episode Reward: 166.0\n",
            "Step 5953, Episode Reward: 23.0\n",
            "Step 6086, Episode Reward: 133.0\n",
            "Step 6243, Episode Reward: 157.0\n",
            "Step 6347, Episode Reward: 104.0\n",
            "Step 6414, Episode Reward: 67.0\n",
            "Step 6513, Episode Reward: 99.0\n",
            "Step 6588, Episode Reward: 75.0\n",
            "Step 6649, Episode Reward: 61.0\n",
            "Step 6823, Episode Reward: 174.0\n",
            "Step 6948, Episode Reward: 125.0\n",
            "Step 7078, Episode Reward: 130.0\n",
            "Step 7181, Episode Reward: 103.0\n",
            "Step 7258, Episode Reward: 77.0\n",
            "Step 7400, Episode Reward: 142.0\n",
            "Step 7472, Episode Reward: 72.0\n",
            "Step 7630, Episode Reward: 158.0\n",
            "Step 7753, Episode Reward: 123.0\n",
            "Step 7779, Episode Reward: 26.0\n",
            "Step 7889, Episode Reward: 110.0\n",
            "Step 7914, Episode Reward: 25.0\n",
            "Step 8080, Episode Reward: 166.0\n",
            "Step 8222, Episode Reward: 142.0\n",
            "Step 8299, Episode Reward: 77.0\n",
            "Step 8520, Episode Reward: 221.0\n",
            "Step 8781, Episode Reward: 261.0\n",
            "Step 8829, Episode Reward: 48.0\n",
            "Step 8962, Episode Reward: 133.0\n",
            "Step 9090, Episode Reward: 128.0\n",
            "Step 9133, Episode Reward: 43.0\n",
            "Step 9304, Episode Reward: 171.0\n",
            "Step 9434, Episode Reward: 130.0\n",
            "Step 9589, Episode Reward: 155.0\n",
            "Step 9851, Episode Reward: 262.0\n",
            "Step 9992, Episode Reward: 141.0\n"
          ]
        }
      ]
    }
  ]
}